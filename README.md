# ğŸ¦™ AlpacaTune: Low-Code LLM Fine-Tuning

**Efficient instruction tuning for large language models with minimal code and maximum impact.**

AlpacaTune demonstrates how to fine-tune the open-source [Mistral-7B-Instruct](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2) LLM using [Ludwig](https://ludwig.ai/) and [LoRA (Low-Rank Adaptation)](https://arxiv.org/abs/2106.09685) for efficient, resource-conscious instruction tuning with just a few lines of code.

---

## ğŸŒŸ Key Features

- ğŸ¯ **Low-Code Approach**: Fine-tune state-of-the-art LLMs with minimal configuration
- ğŸ“š **Instruction Tuning Dataset**: Trained on 5,000 high-quality examples from the [Alpaca dataset](https://github.com/tatsu-lab/alpaca)
- âš¡ **Parameter-Efficient**: Uses 4-bit quantization + LoRA adapters for memory-efficient training on consumer GPUs
- ğŸ”§ **Production-Ready**: Structured evaluation pipeline with real-world test cases
- ğŸš€ **Easy Deployment**: One-command model upload to Hugging Face Hub

---

## ğŸ› ï¸ Tech Stack

| Component | Technology |
|-----------|------------|
| **Framework** | Ludwig 0.10.0 + ludwig[llm] |
| **Base Model** | Mistral-7B-Instruct-v0.2 |
| **Fine-Tuning** | QLoRA (4-bit quantization) |
| **Optimization** | Paged AdamW, Cosine LR Scheduler |
| **Deployment** | Hugging Face Hub Integration |

---

## ğŸš€ Quick Start

### 1. Installation

```bash
pip install ludwig==0.10.0 ludwig[llm] torch==2.1.2 PyYAML==6.0 datasets==2.18.0 pandas==2.1.4
```

### 2. Prepare Dataset

```python
from datasets import load_dataset

# Load and prepare Alpaca dataset
df = load_dataset("tatsu-lab/alpaca")["train"].to_pandas()
df = df[["instruction", "input", "output"]]
print(f"Dataset size: {len(df)} examples")
```

### 3. Configure Fine-Tuning

```yaml
# config.yaml
model_type: llm
base_model: mistralai/Mistral-7B-Instruct-v0.2

input_features:
  - name: instruction
    type: text

output_features:
  - name: output
    type: text

prompt:
  template: >-
    Below is an instruction that describes a task, paired with an input that 
    provides further context. Write a response that appropriately completes the request.
    
    ### Instruction:
    {input.instruction}
    
    ### Input:
    {input.input}
    
    ### Response:

adapter:
  type: lora
  r: 32
  alpha: 64
  dropout: 0.05

quantization:
  bits: 4

trainer:
  type: finetune
  epochs: 1
  batch_size: 1
  eval_batch_size: 2
  learning_rate: 0.0001
  learning_rate_scheduler:
    type: cosine
  optimizer:
    type: paged_adamw
```

### 4. Train the Model

```python
from ludwig.api import LudwigModel

# Initialize and train
model = LudwigModel(config="config.yaml")
results = model.train(dataset=df[:5000])
```

### 5. Test Your Model

```python
# Generate predictions
test_prompts = [
    {"instruction": "Compose a haiku describing summer.", "input": ""},
    {"instruction": "Name two famous 18th century authors.", "input": ""},
    {"instruction": "Explain quantum computing simply.", "input": ""}
]

predictions = model.predict(dataset=test_prompts)
for i, pred in enumerate(predictions):
    print(f"Prompt: {test_prompts[i]['instruction']}")
    print(f"Response: {pred['output']}")
    print("-" * 50)
```

---

## ğŸ“Š Example Results

**Input:** "Compose a haiku describing summer."
```
Golden rays descend
Cicadas sing through the dusk  
Warmth clings to still air
```

**Input:** "Name two famous authors from the 18th century."
```
Jane Austen and Voltaire were influential writers of the 18th century, 
known for their literary contributions and social commentary.
```

**Input:** "Explain the concept of machine learning to a 10-year-old."
```
Machine learning is like teaching a computer to recognize patterns, 
just like how you learn to recognize your friends' faces or 
your favorite songs by hearing them many times.
```

---

## ğŸ¯ Use Cases

- **Content Generation**: Creative writing, summaries, explanations
- **Question Answering**: Domain-specific Q&A systems  
- **Task Automation**: Instruction-following AI assistants
- **Educational Tools**: Personalized tutoring and explanations
- **Code Generation**: Programming assistance and documentation

---

## ğŸ“ˆ Performance & Efficiency

| Metric | Value |
|--------|-------|
| **Training Time** | ~2 hours on single GPU |
| **Memory Usage** | <16GB VRAM (RTX 4090) |
| **Model Size** | ~3.5GB (with LoRA adapters) |
| **Training Examples** | 5,000 instruction-response pairs |
| **Inference Speed** | ~50 tokens/second |

---

## ğŸš€ Deployment

### Deploy to Hugging Face Hub

```bash
# Login to Hugging Face
huggingface-cli login --token <your_token>

# Upload your fine-tuned model
ludwig upload hf_hub \
  --repo_id <your_username>/alpaca-tune-mistral-7b \
  --model_path ./results/experiment_run/model
```

### Local Inference Server

```python
from ludwig.api import LudwigModel

# Load your trained model
model = LudwigModel.load("./results/experiment_run/model")

# Create inference endpoint
def generate_response(instruction, input_text=""):
    prompt = {"instruction": instruction, "input": input_text}
    response = model.predict([prompt])
    return response[0]["output"]

# Use the model
response = generate_response("Write a Python function to calculate fibonacci numbers")
print(response)
```

---

## ğŸ“ Project Structure

```
alpaca-tune/
â”œâ”€â”€ README.md
â”œâ”€â”€ config.yaml              # Fine-tuning configuration
â”œâ”€â”€ train.py                 # Training script
â”œâ”€â”€ evaluate.py              # Evaluation utilities  
â”œâ”€â”€ deploy.py                # Deployment helpers
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ data_exploration.ipynb
â”‚   â”œâ”€â”€ training_demo.ipynb
â”‚   â””â”€â”€ evaluation_results.ipynb
â”œâ”€â”€ data/
â”‚   â””â”€â”€ sample_prompts.json
â””â”€â”€ results/
    â””â”€â”€ experiment_run/
        â”œâ”€â”€ model/
        â”œâ”€â”€ logs/
        â””â”€â”€ predictions/
```

---

## ğŸ¤ Contributing

We welcome contributions! Here's how you can help:

1. **Fork** the repository
2. **Create** a feature branch (`git checkout -b feature/amazing-feature`)
3. **Commit** your changes (`git commit -m 'Add amazing feature'`)
4. **Push** to the branch (`git push origin feature/amazing-feature`)
5. **Open** a Pull Request

---

## ğŸ“ Citation

If you use AlpacaTune in your research or projects, please cite:

```bibtex
@software{alpaca_tune_2024,
  title={AlpacaTune: Low-Code LLM Fine-Tuning},
  author={[Your Name]},
  year={2024},
  url={https://github.com/[your-username]/alpaca-tune},
  note={Fine-tuning Mistral-7B with Ludwig and LoRA}
}
```

---

## ğŸ™ Acknowledgments

- [Mistral AI](https://mistral.ai/) for the excellent base model
- [Ludwig AI](https://ludwig.ai/) for the low-code ML framework  
- [Stanford Alpaca](https://github.com/tatsu-lab/alpaca) for the instruction dataset
- [Microsoft LoRA](https://github.com/microsoft/LoRA) for parameter-efficient fine-tuning

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ“§ Contact

**Interested in LLM fine-tuning, AI agents, or instruction-following models?**

- ğŸ’¼ LinkedIn: [Your LinkedIn Profile]
- ğŸ™ GitHub: [@your-username](https://github.com/your-username)
- ğŸ“§ Email: your.email@domain.com

â­ **Star this repo if you found it helpful!**
